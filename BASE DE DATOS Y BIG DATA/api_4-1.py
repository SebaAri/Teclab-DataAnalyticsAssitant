# -*- coding: utf-8 -*-
"""API-4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12oTR-DxA-4XKJ3wT34fIb23JRmHawCaN
"""

# Instala java y spark
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget http://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
!tar xf spark-3.5.0-bin-hadoop3.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.0-bin-hadoop3"

# Importa las bibliotecas necesarias
import findspark
findspark.init()
from pyspark.sql import SparkSession

# Crea una sesión de Spark
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark.conf.set("spark.sql.repl.eagerEval.enabled", True)

# Importa otras bibliotecas necesarias
from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# Crea una sesión de Spark
spark = SparkSession.builder.appName("consulta-spark").getOrCreate()

# Define un esquema para los datos de ejemplo
schema = StructType([
    StructField("idArticulo", IntegerType(), False),
    StructField("Idnumero", IntegerType(), False),
    StructField("TipoArticulo", StringType(), False),
    StructField("DescripcionArticulo", StringType(), False),
    StructField("Scanning", IntegerType(), False),
    StructField("UxB", IntegerType(), False),
    StructField("StockDiaAnterior", IntegerType(), False)
])

# Crea un DataFrame de Spark con datos de ejemplo
data = [
    (1, 101, 'Tipo1', 'Descripción1', 100, 5, 500),
    (2, 102, 'Tipo2', 'Descripción2', 200, 6, 600),
    (3, 103, 'Tipo3', 'Descripción3', 150, 4, 450)
]

df = spark.createDataFrame(data, schema=schema)

# Registra el DataFrame como una tabla temporal en Spark
df.createOrReplaceTempView("datamart_foto_stock")

# Ejecuta la consulta original
consulta_spark = """
SELECT
    idArticulo AS IDARTICULO,
    Idnumero AS IDNUMERO,
    TipoArticulo AS TIPOARTICULO,
    DescripcionArticulo AS DESCRIPCIONARTICULO,
    Scanning,
    UxB,
    StockDiaAnterior AS STOCKDIAANTERIOR
FROM
    datamart_foto_stock
"""

resultado = spark.sql(consulta_spark)
resultado.show()

# Importa pandas
import pandas as pd
df_vista = spark.table("datamart_foto_stock")

from IPython.display import display
df_pandas = df_vista.toPandas()
display(df_pandas)

# Graba el dataframe en un archivo parquet(sobrescribirá cualquier archivo existente en la ubicación especificada)
df.write.mode("overwrite").parquet("foto_stock.parquet")

# Define el código SQL para crear el esquema y la tabla en Spark
creacion_esquema_tabla = """
CREATE DATABASE IF NOT EXISTS datamart_proveedores;
CREATE TABLE IF NOT EXISTS datamart_proveedores.PROVEEDORES (
    IdProveedor INT PRIMARY KEY,
    NombreProveedor STRING,
    Direccion STRING,
    Telefono STRING,
    Email STRING
)
"""

# Importa las bibliotecas necesarias
from pyspark.sql import SparkSession

# Crea una sesión de Spark
spark = SparkSession.builder.appName("migracion-sql-a-spark").getOrCreate()

# Define el código SQL para crear la tabla en Spark
creacion_tabla = """
CREATE OR REPLACE TEMPORARY VIEW PROVEEDORES AS
SELECT
    1 AS IdProveedor, 'Proveedor1' AS NombreProveedor, 'Mitre 256' AS Direccion, '011-456-7890' AS Telefono, 'proveedor1@yahoo.com' AS Email
UNION ALL
SELECT
    2 AS IdProveedor, 'Proveedor2' AS NombreProveedor, 'Estomba 1203' AS Direccion, '011-654-3210' AS Telefono, 'proveedor2@hotmail.com' AS Email
UNION ALL
SELECT
    3 AS IdProveedor, 'Proveedor3' AS NombreProveedor, 'Urquiza 128' AS Direccion, '0291-922-4633' AS Telefono, 'proveedor3@gmail.com' AS Email
"""

# Ejecuta el código SQL para crear la tabla
spark.sql(creacion_tabla)

# Define la consulta para obtener los datos
consulta_datos = """
SELECT
    IdProveedor AS IDPROVEEDOR,
    NombreProveedor AS NOMBREPROVEEDOR,
    Direccion AS DIRECCION,
    Telefono AS TELEFONO,
    Email AS EMAIL
FROM
    PROVEEDORES
"""

# Ejecuta la consulta para obtener los datos
resultado = spark.sql(consulta_datos)

# Muestra el resultado
resultado.show()

# Importa la librería pandas
import pandas as pd
df_vista1 = spark.table("PROVEEDORES")

from IPython.display import display
df_pandas1 = df_vista1.toPandas()
display(df_pandas1)

# Graba el dataframe en un archivo parquet (sobrescribirá cualquier archivo existente en la ubicación especificada)
df.write.mode("overwrite").parquet("proveedores.parquet")

# Importa las bibliotecas necesarias
from pyspark.sql import SparkSession

# Crea una sesión de Spark
spark = SparkSession.builder.appName("migracion-sql-a-spark").getOrCreate()

# Define el código SQL para crear la tabla en Spark
creacion_tabla = """
CREATE OR REPLACE TEMPORARY VIEW ESCANEO_PRODUCTOS AS
SELECT
    1 AS IdEscaneo, 1 AS idArticulo, 101 AS Idnumero, 'Tipo1' AS TipoArticulo, 'Descripción1' AS DescripcionArticulo, 120 AS Scanning, 5 AS UxB, 550 AS StockActual
UNION ALL
SELECT
    2 AS IdEscaneo, 2 AS idArticulo, 102 AS Idnumero, 'Tipo2' AS TipoArticulo, 'Descripción2' AS DescripcionArticulo, 210 AS Scanning, 6 AS UxB, 590 AS StockActual
UNION ALL
SELECT
    3 AS IdEscaneo, 3 AS idArticulo, 103 AS Idnumero, 'Tipo3' AS TipoArticulo, 'Descripción3' AS DescripcionArticulo, 155 AS Scanning, 4 AS UxB, 455 AS StockActual
"""

# Ejecuta el código SQL para crear la tabla
spark.sql(creacion_tabla)

# Define la consulta para obtener los datos
consulta_datos = """
SELECT
    IdEscaneo AS IDESCANEO,
    idArticulo AS IDARTICULO,
    Idnumero AS IDNUMERO,
    TipoArticulo AS TIPOARTICULO,
    DescripcionArticulo AS DESCRIPCIONARTICULO,
    Scanning,
    UxB,
    StockActual AS STOCKACTUAL
FROM
    ESCANEO_PRODUCTOS
"""

# Ejecuta la consulta para obtener los datos
resultado = spark.sql(consulta_datos)

# Muestra el resultado
resultado.show()

# Importa la librería pandas
import pandas as pd
df_vista2 = spark.table("ESCANEO_PRODUCTOS")

from IPython.display import display
df_pandas2 = df_vista2.toPandas()
display(df_pandas2)

# Guarda el dataframe en un archivo parquet (sobrescribirá cualquier archivo existente en la ubicación especificada)
df.write.mode("overwrite").parquet("Escaneo_productos.parquet")

# Cierra la sesión de Spark cuando haya terminado
spark.stop